---
title: "Comparing mGP and tIWP2"
author: "Ziang Zhang"
date: "2024-06-27"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## **Introduction**

There are two ways to parametrize a GP prior that penalizes the function $f$ toward the monotone base model $\text{span}\{1,m(x)\}$ where $m(x)$ is a monotonically increasing function such that $m'(x)>0$ for all $x \in \Omega$.

The first way is to use the M-GP prior. Let $\alpha(x) = \frac{m''(x)}{m'(x)}$ denotes the relative curvature function, then the M-GP prior is defined as
$$
L_\alpha f(x) := D^2 f(x) - \alpha(x) D f(x) = \sigma\xi(x).
$$

An alternative method is to use the transformed IWP2 prior (t-IWP2). 
The IWP-2 is defined as:
$$
D^2 \tilde{f}(x^*) = \sigma\xi(x^*).
$$
So it has base model being $\text{span}\{1, x^*\}$. 
By transforming the coordinate $x^* = m(x)$, we can obtain the t-IWP2 prior as $f(x) = \tilde{f}(m(x))$. 
The base model of t-IWP2 is therefore $\text{span}\{1, m(x)\}$, which is the same as the M-GP prior.


## **SDE characterization of the t-IWP2**

To conceptually compare the two priors, we can consider the SDE characterization of the t-IWP2 prior.

\begin{equation}
\begin{aligned}
f'(x) &= \tilde{f}'(m(x)) m'(x).\\
f''(x) &= \tilde{f}''(m(x)) m'(x)^2 + \tilde{f}'(m(x)) m''(x) \\
&= \tilde{f}''\left(m(x)\right) m'(x)^2 + f'(x) \frac{m''(x)}{m'(x)} \\
&= \tilde{f}''\left(m(x)\right) m'(x)^2 + f'(x) \alpha(x).
\end{aligned}
\end{equation}
Therefore we have:
$$L_\alpha f(x) = \tilde{f}''\left(m(x)\right) m'(x)^2.$$

Note that $\tilde{f}''\left(m(x)\right) m'(x)^2 = \tilde{f}''\left(m(x)\right) m'(x)^{1/2} m'(x)^{3/2}$, and that $\tilde{f}''\left(m(x)\right) m'(x)^{1/2} = \sigma\xi(x)$ (*Claim 1*, proof attached later). We can then write the SDE of the t-IWP2 prior as:
$$
L_\alpha f(x) = [\sigma m'(x)^{3/2}]\xi(x).
$$


Equivalently, we can move the extra $m'(x)^{3/2}$ to the left hand side and define a new operator $L_\alpha^* = m'(x)^{-3/2} L_\alpha$ and then $L_\alpha^*f(x) = \sigma\xi(x)$. 
It is then obvious that the t-IWP2 prior is (almost) equivalent to the M-GP prior, except the scaling factor $\sigma m'(x)^{-3/2}$ on the differential operator.
The effect of this scaling factor essentially changes the constant $\sigma$ in the M-GP prior to a function of $x$ in the t-IWP2 prior (i.e. $\sigma(x) = [\sigma m'(x)^{3/2}]$).
This implies comparing to the M-GP prior, the t-IWP2 prior has a more flexible penalty strength that can vary with $x$.
When the function is changing fast ($m'$ is large), the t-IWP2 prior has a small penalty and allows the function to deviate more from the base model than M-GP.


### ***Proof of Claim 1***

Without the loss of generality, let's assume $\sigma = 1$ and let $x^* = m(x)$.
To show $\tilde{f}''\left(m(x)\right) m'(x)^{1/2} = \xi(x)$, we first notice that $\tilde{f}''\left(m(x)\right) m'(x)^{1/2} = \xi(m(x)) m'(x)^{1/2}$ as $\tilde{f}$ is an IWP2.

For any $f_1, f_2 \in L^2$, we have:
\begin{equation}
\begin{aligned}
\int \xi(m(x)) m'(x)^{1/2} f_1(x) dx &= \int \xi(x^* ) m'(m^{-1}(x^* ))^{-1/2} f_1(m^{-1}(x^* ))  dx^*
\end{aligned}
\end{equation}

Therefore, using property of Gaussian white noise $\xi(x^*)$, we have:
\begin{equation}
\begin{aligned}
\text{Cov}\bigg[\int\xi(m(x)) m'(x)^{1/2} f_1(x)dx, \int \xi(m(x)) m'(x)^{1/2} f_2(x)dx  \bigg]
&= \text{Cov}\bigg[\int\xi(x^* ) m'(m^{-1}(x^* ))^{-1/2} f_1(m^{-1}(x^* ))dx^* , \int \xi(x^* ) m'(m^{-1}(x^* ))^{-1/2} f_2(m^{-1}(x^* ))dx^*  \bigg] \\
&= \int m'(m^{-1}(x^* ))^{-1} f_1(m^{-1}(x^* )) f_2(m^{-1}(x^* )) dx^* \\
&= \int f_1(x) f_2(x) dx \\
&= \text{Cov}\bigg[\int\xi(x) f_1(x)dx, \int \xi(x) f_2(x)dx  \bigg].
\end{aligned}
\end{equation}
The proof is hence complete.



## **Comparison of samples**

### Case 1: When $m(x) = \sqrt{x+c}$

```{r message=FALSE, warning=FALSE, echo=TRUE}
library(tidyverse)
library(Matrix)
source("code/01-state-space.R")
source("code/02-FEM.R")
source("code/03-sampling.R")
```


To compare the two priors, we can simulate samples from the two priors and compare the samples when $m(x) = \sqrt x$.

For the M-GP prior:
```{r}
B = 100
c <- 1.1
m <- function(x) {sqrt(x+c)}
m_deriv <- function(x) {1/(2*sqrt(x+c))}

samps0 <- mGP_sim(mesh_size = 0.1, max_t = 10, alpha = (2), c = c, initial_vec = c(m(0),m_deriv(0)), sd = 0.1)
result0 <- samps0[,1:2]
for (i in 1:B) {
  samps0 <- mGP_sim(mesh_size = 0.1, max_t = 10, alpha = (2), c = c, initial_vec = c(m(0),m_deriv(0)), sd = 0.1)
  result0 <- cbind(result0, samps0[,2])
}
# result0[1:nrow(result0),-1] <- result0[1:nrow(result0),-1]/sd(result0[nrow(result0),-1])
matplot(x = result0[,1], y = result0[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "alpha = 2", ylim = c(0,8), lty = "dashed", col = "pink")
lines(x = result0[,1], y = m(result0[,1]), col = "black", lty = "solid")
```

For the t-IWP2 prior:
```{r}
# set the original grid
t <- seq(0, 10, by = 0.1)
# compute the transformed grid
t_trans <- m(t)

# sampling from tIWP2
samps1 <- sim_IWp_Var(t = t_trans, p = 2, sd = 0.1, initial_vec = c(m(0),1))
result1 <- cbind(c(t), samps1[,2])
for (i in 1:B) {
  samps1 <- sim_IWp_Var(t = t_trans, p = 2, sd = 0.1, initial_vec = c(m(0),1))
  result1 <- cbind(result1, samps1[,2])
}
# result1[1:nrow(result1),-1] <- result1[1:nrow(result1),-1]/sd(result1[nrow(result1),-1])
matplot(x = result1[,1], y = result1[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "tIWP2", ylim = c(0,8), lty = "dashed", col = "pink")
lines(x = result1[,1], y = m(result1[,1]), col = "black", lty = "solid")
```

Compare the variance of the two priors:
```{r}
## Variance of overtime:
var0 <- apply(result0[,-1], 1, var)
var1 <- apply(result1[,-1], 1, var)
plot(x = result0[,1], y = var0, type = 'l', ylab = "Variance", xlab = "x", main = "Variance of samples", ylim = c(0,5), lty = "dashed", col = "red")
lines(x = result1[,1], y = var1, col = "blue", lty = "solid")
legend("topright", legend=c("M-GP", "tIWP2"), col=c("red", "blue"), lty=1:1, cex=1)
```

Let's standardize by the variance at time 2 for each method:
```{r}
var0 <- apply(result0[,-1], 1, var)/var(result0[which(result0[,1] == 2),-1])
var1 <- apply(result1[,-1], 1, var)/var(result1[which(result1[,1] == 2),-1])
plot(x = result0[,1], y = var0, type = 'l', ylab = "Variance", xlab = "x", main = "Variance of samples (standardized at time 2)", lty = "dashed", col = "red")
lines(x = result1[,1], y = var1, col = "blue", lty = "solid")
legend("topright", legend=c("M-GP", "tIWP2"), col=c("red", "blue"), lty=1:1, cex=1)
```

```{r}
plot(x = result0[-1,1], y = var0[-1], type = 'l', ylab = "Variance", xlab = "x", main = "Variance of samples (standardized at time 2)", lty = "dashed", col = "red", log = "y")
lines(x = result1[-1,1], y = var1[-1], col = "blue", lty = "solid")
legend("bottomright", legend=c("M-GP", "tIWP2"), col=c("red", "blue"), lty=1:1, cex=1)
```


### Case 2: When $m(x) = (x+c)^3$


```{r}
m <- function(x) {(x+c)^3}
m_deriv <- function(x) {3*(x+c)^2}
samps0 <- mGP_sim(mesh_size = 0.1, max_t = 10, alpha = (-1/2), c = c, initial_vec = c(m(0),m_deriv(0)), sd = 0.1)
result0 <- samps0[,1:2]
for (i in 1:B) {
  samps0 <- mGP_sim(mesh_size = 0.1, max_t = 10, alpha = (-1/2), c = c, initial_vec = c(m(0),m_deriv(0)), sd = 0.1)
  result0 <- cbind(result0, samps0[,2])
}
matplot(x = result0[,1], y = result0[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "alpha = -1/2", lty = "dashed", col = "pink")
lines(x = result0[,1], y = m(result0[,1]), col = "black", lty = "solid")
```

```{r}
# set the original grid
t <- seq(0, 10, by = 0.1)
# compute the transformed grid
t_trans <- m(t)
# sampling from tIWP2
samps1 <- sim_IWp_Var(t = t_trans, p = 2, sd = 0.1, initial_vec = c(m(0),1))
result1 <- cbind(c(t), samps1[,2])
for (i in 1:B) {
  samps1 <- sim_IWp_Var(t = t_trans, p = 2, sd = 0.1, initial_vec = c(m(0),1))
  result1 <- cbind(result1, samps1[,2])
}
matplot(x = result1[,1], y = result1[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "tIWP2", lty = "dashed", col = "pink")
lines(x = result1[,1], y = m(result1[,1]), col = "black", lty = "solid")
```

Compare the variance of the two priors:
```{r}
## Variance of overtime:
var0 <- apply(result0[,-1], 1, var)
var1 <- apply(result1[,-1], 1, var)
plot(x = result0[,1], y = var0, type = 'l', ylab = "Variance", xlab = "x", main = "Variance of samples", ylim = c(0,1000), lty = "dashed", col = "red")
lines(x = result1[,1], y = var1, col = "blue", lty = "solid")
legend("topright", legend=c("M-GP", "tIWP2"), col=c("red", "blue"), lty=1:1, cex=1)
```



Let's standardize by the variance at time 2 for each method:
```{r}
var0 <- apply(result0[,-1], 1, var)/var(result0[which(result0[,1] == 2),-1])
var1 <- apply(result1[,-1], 1, var)/var(result1[which(result1[,1] == 2),-1])
plot(x = result0[,1], y = var0, type = 'l', ylab = "Variance", xlab = "x", main = "Variance of samples (standardized at time 2)", ylim = c(0,1000), lty = "dashed", col = "red")
lines(x = result1[,1], y = var1, col = "blue", lty = "solid")
legend("topright", legend=c("M-GP", "tIWP2"), col=c("red", "blue"), lty=1:1, cex=1)
```


## **Constraint when $x \neq 0$ **

### **A naive try using M-GP**

Here let's consider what happens when we have information about the function $f$ at $x \neq 0$. 
Specifically, let's assume we have a function $f$ defines for $t \in [0.2,11]$, where $f(9)$ is known to be 0 and $f'(9)$ is known to be 0.002.

To make the concept of M-GP clear, let's assume $f$ behaves like a square root function, so the base model is $\text{span}\{1, \sqrt{x}\}$.

The default condition of M-GP is the initial condition at $x = 0$, but here we have information on $x = 9$. So let's define $\tilde{x} = x - c$ where $c = 9$ for $x \geq 9$. Similar for $x \leq c$, we define $\underset{\sim}{x} = c - x$.

Then, we define two independent M-GP priors for $x \geq c$ and $x \leq c$ respectively.

```{r}
B <- 100
ref <- 9
sd = 0.0001

t <- seq(0.1, 11, by = 0.1)
x <- t - ref
m <- function(x) {sqrt(x+ref)}
m_deriv <- function(x) {1/(2*sqrt(x+ref))}
f_condition <- 0
f_deriv_condition <- 0.002

tilde_x <- sort(unique(ifelse(x >= 0, x, 0)))
x_tilde <- sort(unique(ifelse(x <= 0, -x, 0)))

samps_pos <- mGP_sim(t = tilde_x, alpha = (2), c = ref, initial_vec = c(f_condition, f_deriv_condition), sd = sd)
result_pos <- samps_pos[,1:2]
for (i in 1:B) {
  samps_pos <- mGP_sim(t = tilde_x, alpha = (2), c = ref, initial_vec = c(f_condition, f_deriv_condition), sd = sd)
  result_pos <- cbind(result_pos, samps_pos[,2])
}

samps_neg <- adj_mGP_sim(t = x_tilde, alpha = (2), c = ref, initial_vec = c(f_condition, -f_deriv_condition), sd = sd)
result_neg <- samps_neg[,1:2]
for (i in 1:B) {
  samps_neg <- adj_mGP_sim(t = x_tilde, alpha = (2), c = ref, initial_vec = c(f_condition, -f_deriv_condition), sd = sd)
  result_neg <- cbind(result_neg, samps_neg[,2])
}

# set the original grid
result_neg[,1] <- ref - result_neg[,1]
result_pos[,1] <- result_pos[,1] + ref

result <- rbind(result_neg[-1,], result_pos)
# sort the result
result <- result[order(result[,1]),]

# solve b1 b2 such that b1 + b2*sqrt(ref) = 0 and b2*1/(2*sqrt(ref)) = 0.002 when ref = 9
A <- matrix(c(1, sqrt(9), 0, 1/(2*sqrt(9))), nrow = 2, byrow = TRUE)
b <- solve(A) %*% c(f_condition, f_deriv_condition)


## the true m:
m_true <- function(x) b[1] + b[2]*sqrt(x)

matplot(x = result[,1], y = result[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "alpha = 2", lty = "dashed", col = "pink")
        # , ylim = c(-0.03, 0.01))
lines(x = result[,1], y = m_true(result[,1]), col = "black", lty = "solid")
```


The result does not look right...

Because $\sqrt{1-x} \neq b_1 + b_2\sqrt{x}$ for some $b_1, b_2$.
In other words, if $f(x)$ is a M-GP, $f(-x)$ is not a M-GP.
For example, when $f(x) = \sqrt{x+c}$ and $\tilde{x} = x - c_1$, we can define $f_1(\tilde{x}) = \sqrt{\tilde{x} + c_1 +c} = \sqrt{(x-c_1)+c_1 + c} = \sqrt{x+c} = f(x)$.
But if we define $\underset{\sim}{x} = c_1-x$, then $f_2(\underset{\sim}{x}) = \sqrt{c_1+c-\underset{\sim}{x}} = \sqrt{x+c} = f(x)$. 
The form of $\sqrt{c_1+c-\underset{\sim}{x}}$ can not be written as the base model of any M-GP!

We need to find which model $f(-x)$ should correspond to...



### **The reverse M-GP process**

Given a choice of $\alpha_c(x) = -1/a(x+c)$, let's define a new function $\alpha^*_{c}(x) := [-1/a(x-c)] = 1/a(c-x) = -\alpha_c(-x)$ and a new operator $\tilde{L}^{*}_\alpha$ such that $\tilde{L}^{*}_\alpha = D^2 - \alpha^*_{c}(x)D$.

We call the process generated by this operator the **reverse M-GP**.

```{r}
source("code/04-state-space-adjoint.R")
source("code/05-sampling-adjoint.R")
```

```{r}
B <- 100
c <- 0.2
c1 <- 8.8
sd <- 0.0001

t <- seq(0.2, 11, by = 0.1)
x <- t - c
m <- function(x) {sqrt(x+c)}
m_deriv <- function(x) {1/(2*sqrt(x+c))}
f_condition <- 0
f_deriv_condition <- 0.002

tilde_x <- sort(unique(ifelse(x >= c1, x-c1, 0)))
x_tilde <- sort(unique(ifelse(x <= c1, c1-x, 0)))

samps_pos <- mGP_sim(t = tilde_x, alpha = (2), c = (c + c1), initial_vec = c(f_condition, f_deriv_condition), sd = sd)
result_pos <- samps_pos[,1:2]
for (i in 1:B) {
  samps_pos <- mGP_sim(t = tilde_x, alpha = (2), c = (c + c1), initial_vec = c(f_condition, f_deriv_condition), sd = sd)
  result_pos <- cbind(result_pos, samps_pos[,2])
}

samps_neg <- adj_mGP_sim(t = x_tilde, alpha = (2), c = (c + c1), initial_vec = c(f_condition, -f_deriv_condition), sd = sd)
result_neg <- samps_neg[,1:2]
for (i in 1:B) {
  samps_neg <- adj_mGP_sim(t = x_tilde, alpha = (2), c = (c + c1), initial_vec = c(f_condition, -f_deriv_condition), sd = sd)
  result_neg <- cbind(result_neg, samps_neg[,2])
}

# set the original grid
result_neg[,1] <- c1 - result_neg[,1]
result_pos[,1] <- result_pos[,1] + c1

result <- rbind(result_neg[-1,], result_pos)
# sort the result
result <- result[order(result[,1]),]
result[,1] <- result[,1] + c

# solve b1 b2 such that b1 + b2*sqrt(x+c) = 0 and b2*1/(2*sqrt(x+c)) = 0.002 when x = 8.8
A <- matrix(c(1, sqrt(9), 0, 1/(2*sqrt(9))), nrow = 2, byrow = TRUE)
b <- solve(A) %*% c(f_condition, f_deriv_condition)

## the true m:
m_true <- function(x) b[1] + b[2]*sqrt(x)

matplot(x = result[,1], y = result[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "alpha = 2", lty = "dashed", col = "pink", ylim = c(-0.03, 0.01))
lines(x = result[,1], y = m_true(result[,1]), col = "black", lty = "solid")
```

Now, the result looks correct!


### **Using t-IWP2**

Now, how should we use t-IWP2 in the same case?

This becomes an easier problem, as the reverse process of IWP2 is still an IWP2 with an opposite derivative (i.e. $f(x) \sim \text{IWP}$ implies $f(-x) \sim \text{IWP}$).

This implies that we just need to merge the forward IWP and the backward IWP together, while being careful with the initial condition ($f'(c) = \tilde{f}'(m(c))m'(c)$, so $\tilde{f}'(m(c)) = f'(c)/m'(c)$).

```{r}
B <- 100
c <- 0.2
c1 <- 8.8
sd = 0.001

m <- function(x){sqrt(x + c)} 
m_deriv <- function(x){1/(2*sqrt(x + c))}

x <- seq(0, 10.8, by = 0.1)
x_trans <- m(x)

x_trans_part1 <- x_trans[x_trans <= 3] # 3 = m(c1)
x_trans_part2 <- x_trans[x_trans >= 3]

x_trans_part1_relative <- rev(max(x_trans_part1) - x_trans_part1) 
# take rev(..) to make the sequence increasing.
x_trans_part2_relative <- x_trans_part2 - min(x_trans_part2)
# this is an increasing sequence, no need to reverse it.

samps1_part1 <- sim_IWp_Var(t = x_trans_part1_relative, p = 2, sd = sd, 
                      initial_vec = c(f_condition,-f_deriv_condition/m_deriv(c1)))
result1_part1 <- cbind(c(x[x_trans <= 3]), samps1_part1[,2])
for (i in 1:B) {
  samps1_part1 <- sim_IWp_Var(t = x_trans_part1_relative, p = 2, sd = sd, 
                      initial_vec = c(f_condition,-f_deriv_condition/m_deriv(c1)))
  result1_part1 <- cbind(result1_part1, samps1_part1[,2])
}
result1_part1[,1] <- c + c1 - result1_part1[,1]
# matplot(x = result1_part1[,1], y = result1_part1[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "alpha = 2", lty = "dashed", col = "pink", ylim = c(-0.03, 0.01))
# lines(x = result1_part1[,1], y = m_true(result1_part1[,1]), col = "black", lty = "solid")

samps1_part2 <- sim_IWp_Var(t = x_trans_part2_relative, p = 2, sd = sd, 
                      initial_vec = c(f_condition, f_deriv_condition/m_deriv(c1)))
result1_part2 <- cbind(c(x[x_trans >= 3]), samps1_part2[,2])
for (i in 1:B) {
  samps1_part2 <- sim_IWp_Var(t = x_trans_part2_relative, p = 2, sd = sd, 
                      initial_vec = c(f_condition, f_deriv_condition/m_deriv(c1)))
  result1_part2 <- cbind(result1_part2, samps1_part2[,2])
}
result1_part2[,1] <- c + result1_part2[,1]
# matplot(x = result1_part2[,1], y = result1_part2[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "alpha = 2", lty = "dashed", col = "pink")
# lines(x = result1_part2[,1], y = m_true(result1_part2[,1]), col = "black", lty = "solid")
```

Merge the two t-IWP2 together, we get the final result!

```{r}
result <- rbind(result1_part1[-1,], result1_part2)
# sort the result
result <- result[order(result[,1]),]

matplot(x = result[,1], y = result[,-1], type = 'l', ylab = "Post Samp", xlab = "x", main = "alpha = 2", lty = "dashed", col = "pink", ylim = c(-0.03, 0.01))
lines(x = result[,1], y = m_true(result[,1]), col = "black", lty = "solid")
```




### **Which one is easier?**

So when we want to condition at some value $x \neq 0$, which method is easier to use?

Overall, the t-IWP2 is easier, as it does not require the computation of the reverse process. The only additional step required would be to split the domain and fit two independent IWP2 processes.

To use the M-GP, we need to compute the reverse M-GP process. 
The functions in `code/04-state-space-adjoint.R` and `code/05-sampling-adjoint.R` are designed to compute the reverse process of M-GP for a range of $a$ values.
However, the computation of the reverse M-GP process changes with the choice of $a$ in complicated way, and there are still cases of $a$ that are not covered by the current implementation.
Generally speaking, the computation of reverse M-GP process for arbitrary $a$ should be done with manual computation for each $a$, which is not hard, but a bit tedious.











